{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Dict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (121740, 4)\n",
      "Test data size: (7189, 4)\n",
      "Validation data size: (7165, 4)\n",
      "Classes distribution:\n",
      "1    89389\n",
      "0    32351\n",
      "Name: Label, dtype: int64\n",
      "Some train examples:                                             Claim  \\\n",
      "0  Chris Hemsworth appeared in A Perfect Getaway.   \n",
      "1                         Roald Dahl is a writer.   \n",
      "2                       Roald Dahl is a governor.   \n",
      "\n",
      "                                            Evidence  ID  Label  \n",
      "0  2\\tHemsworth has also appeared in the science ...   3      1  \n",
      "1  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   7      1  \n",
      "2  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   8      0  \n",
      "Some test examples:                                                Claim  \\\n",
      "0    Anxiety has been linked with physical symptoms.   \n",
      "1                         Firefox is an application.   \n",
      "2  Keegan-Michael Key played President Barack Oba...   \n",
      "\n",
      "                                            Evidence     ID  Label  \n",
      "0  13\\tFurthermore , anxiety has been linked with...  16387      1  \n",
      "1  0\\tMozilla Firefox -LRB- or simply Firefox -RR...      6      1  \n",
      "2  6\\tIn 2015 , Key appeared at the White House C...  16392      1  \n",
      "Some validation examples:                                                Claim  \\\n",
      "0  The Indian Army comprises part of the country'...   \n",
      "1                         Recovery features Rihanna.   \n",
      "2                            Rihanna is on Recovery.   \n",
      "\n",
      "                                            Evidence     ID  Label  \n",
      "0  16\\tIt is an all-volunteer force and comprises...  98304      1  \n",
      "1  6\\tEminem also collaborated with artists such ...  98305      1  \n",
      "2  6\\tEminem also collaborated with artists such ...  98306      1  \n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "trainData = pd.read_csv('./fever_data/train_pairs.csv')\n",
    "testData = pd.read_csv('./fever_data/test_pairs.csv')\n",
    "valData = pd.read_csv('./fever_data/val_pairs.csv')\n",
    "\n",
    "#drop first column\n",
    "trainData = trainData.drop(trainData.columns[0], axis=1)\n",
    "valData = valData.drop(valData.columns[0], axis=1)\n",
    "testData = testData.drop(testData.columns[0], axis=1)\n",
    "\n",
    "#transfer label into 0/1\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(trainData['Label'])\n",
    "trainData['Label'] =labelencoder.transform(trainData['Label'])\n",
    "valData['Label'] =labelencoder.transform(valData['Label'])\n",
    "testData['Label'] =labelencoder.transform(testData['Label'])\n",
    "\n",
    "print(\"Training data size: {}\".format(trainData.shape))\n",
    "print(\"Test data size: {}\".format(testData.shape))\n",
    "print(\"Validation data size: {}\".format(valData.shape))\n",
    "print(\"Classes distribution:\\n{}\".format(trainData.Label.value_counts())) \n",
    "\n",
    "print(\"Some train examples: {}\".format(trainData.iloc[:3]))\n",
    "print(\"Some test examples: {}\".format(testData.iloc[:3]))\n",
    "print(\"Some validation examples: {}\".format(valData.iloc[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n",
      "[Debug] Before:\n",
      "                                            Claim  \\\n",
      "0  Chris Hemsworth appeared in A Perfect Getaway.   \n",
      "1                         Roald Dahl is a writer.   \n",
      "2                       Roald Dahl is a governor.   \n",
      "\n",
      "                                            Evidence  ID  Label  \n",
      "0  2\\tHemsworth has also appeared in the science ...   3      1  \n",
      "1  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   7      1  \n",
      "2  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   8      0  \n",
      "[Debug] After:\n",
      "                                               Claim  \\\n",
      "0  [chris, hemsworth, appeared, a, perfect, getaway]   \n",
      "1                              [roald, dahl, writer]   \n",
      "2                            [roald, dahl, governor]   \n",
      "\n",
      "                                            Evidence  ID  Label  \n",
      "0  [hemsworth, also, appeared, science, fiction, ...   3      1  \n",
      "1  [roald, dahl, 13, september, 1916, 23, novembe...   7      1  \n",
      "2  [roald, dahl, 13, september, 1916, 23, novembe...   8      0  \n",
      "\n",
      "Pre-processing completed!\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.:`\\-\\'\\\"]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z #+_\\|@,;.:`\\-\\'\\\"\\\\\\/]')\n",
    "REMOVE_SB = re.compile('-LSB-(.*?)-RSB-')\n",
    "REMOVE_RB = re.compile('-LRB-|-RRB-')\n",
    "RB_PAIRS = re.compile('-LRB-(.*?)-RRB-') \n",
    "\n",
    "\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    Example:\n",
    "    Input: 'I really like New York city'\n",
    "    Output: 'i really like new your city'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces special characters, such as paranthesis,\n",
    "    with spacing character\n",
    "    \"\"\"\n",
    "\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def replace_br(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces br characters\n",
    "    \"\"\"\n",
    "\n",
    "    return text.replace('br', '')\n",
    "\n",
    "def remove_SB_text(text):\n",
    "    \"\"\"\n",
    "    Removes -LSB- and -RSB- pairs in the text\n",
    "    \"\"\"\n",
    "    return REMOVE_SB.sub('', text)\n",
    "\n",
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the\n",
    "    good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_RB_text(text):\n",
    "    \"\"\"\n",
    "    Removes -LRB- -RRB- or -LRB- -RRB- pairs in the text\n",
    "    \"\"\"\n",
    "    sentences = re.findall(RB_PAIRS, text)\n",
    "    for sent in sentences: \n",
    "        if re.search(GOOD_SYMBOLS_RE, sent) is not None:\n",
    "            text = RB_PAIRS.sub('', text, 1)\n",
    "        else:\n",
    "            text = REMOVE_RB.sub('', text, 2)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    Example:\n",
    "    Input: '  This assignment is cool\\n'\n",
    "    Output: 'This assignment is cool'\n",
    "    \"\"\"\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def split_text(text: str) -> str:\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          remove_SB_text,\n",
    "                          remove_RB_text,\n",
    "                          replace_special_characters,\n",
    "                          replace_br,\n",
    "                          filter_out_uncommon_symbols,\n",
    "                          remove_stopwords,\n",
    "                          lower,\n",
    "                          strip_text,\n",
    "                          split_text\n",
    "                          ]\n",
    "\n",
    "\n",
    "def text_prepare(text: str,filter_methods=PREPROCESSING_PIPELINE):\n",
    "    return reduce(lambda x, f: f(x), filter_methods, text)\n",
    "\n",
    "print('Pre-processing text...')\n",
    "print('[Debug] Before:\\n{}'.format(trainData[:3]))\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "trainData.Claim = trainData.Claim.apply(lambda x: text_prepare(x))\n",
    "trainData.Evidence = trainData.Evidence.apply(lambda x: x.split('\\t')[1])\n",
    "trainData.Evidence = trainData.Evidence.apply(lambda x: text_prepare(x))\n",
    "\n",
    "testData.Claim = testData.Claim.apply(lambda x: text_prepare(x))\n",
    "testData.Evidence = testData.Evidence.apply(lambda x: x.split('\\t')[1])\n",
    "testData.Evidence = testData.Evidence.apply(lambda x: text_prepare(x))\n",
    "\n",
    "valData.Claim = valData.Claim.apply(lambda x: text_prepare(x))\n",
    "valData.Evidence = valData.Evidence.apply(lambda x: x.split('\\t')[1])\n",
    "valData.Evidence = valData.Evidence.apply(lambda x: text_prepare(x))\n",
    "\n",
    "\n",
    "print('[Debug] After:\\n{}'.format(trainData[:3]))\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train claim data:  (121740,)\n",
      "Train evidence data:  (121740,)\n",
      "Train label:  (121740,)\n",
      "Validation claim data:  (7165,)\n",
      "Validation evidence data:  (7165,)\n",
      "Validation label:  (7165,)\n",
      "Test claim data:  (7189,)\n",
      "Test evidence data:  (7189,)\n",
      "Test label:  (7189,)\n"
     ]
    }
   ],
   "source": [
    "x_train_Claim = trainData.Claim.values\n",
    "x_train_Evidence = trainData.Evidence.values\n",
    "y_train = trainData.Label.values\n",
    "x_test_Claim = testData.Claim.values\n",
    "x_test_Evidence = testData.Evidence.values\n",
    "y_test = testData.Label.values\n",
    "x_val_Claim = valData.Claim.values\n",
    "x_val_Evidence = valData.Evidence.values\n",
    "y_val = valData.Label.values\n",
    "\n",
    "print('Train claim data: ', x_train_Claim.shape)\n",
    "print('Train evidence data: ', x_train_Evidence.shape)\n",
    "print('Train label: ', y_train.shape)\n",
    "print('Validation claim data: ', x_val_Claim.shape)\n",
    "print('Validation evidence data: ', x_val_Evidence.shape)\n",
    "print('Validation label: ', y_val.shape)\n",
    "print('Test claim data: ', x_test_Claim.shape)\n",
    "print('Test evidence data: ', x_test_Evidence.shape)\n",
    "print('Test label: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257810/257810 [00:00<00:00, 608957.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index -> Word vocabulary size: 32063\n",
      "Word -> Index vocabulary size: 32063\n",
      "The corpus length: 32063\n",
      "Some words: [('hemsworth', 1), ('appeared', 2), ('a', 3), ('perfect', 4), ('getaway', 5), ('roald', 6), ('dahl', 7), ('writer', 8), ('governor', 9), ('ireland', 10)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function definition\n",
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],Dict[str, int],List[str]):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    curr_idx = 0\n",
    "    \n",
    "    for sentence in tqdm(df.values):\n",
    "        tokens = sentence\n",
    "        for token in tokens:\n",
    "          if token not in word_to_idx:\n",
    "              word_to_idx[token] = curr_idx\n",
    "              idx_to_word[curr_idx] = token\n",
    "              curr_idx += 1\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing\n",
    "\n",
    "\n",
    "corpus = pd.concat([trainData.Claim,trainData.Evidence,valData.Claim,valData.Evidence],ignore_index=True)\n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(corpus)\n",
    "\n",
    "print('Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\n",
    "print('Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\n",
    "print('The corpus length: {}'.format(len(word_listing)))\n",
    "print('Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(10) + 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 300\n",
    "download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "\n",
    "try:\n",
    "    embedding_model = gloader.load(download_path)\n",
    "except ValueError as e:\n",
    "    print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "    print(\"Glove: 50, 100, 200, 300\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 2261 (7.05%)\n"
     ]
    }
   ],
   "source": [
    "# define function to check oov terms  \n",
    "\n",
    "def check_OOV_terms(embedding_model,word_listing):\n",
    "  \n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    #oov = [text for text in word_listing if text not in embedding_vocabulary]\n",
    "    return list(oov)\n",
    "\n",
    "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
    "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary length: 32065\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "oov_token = \"<OOV>\"\n",
    "tokenizer = Tokenizer(oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "vocabLength = len(tokenizer.word_index)+1 \n",
    "print(\"Tokenizer vocabulary length: {}\".format(vocabLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulid the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32064/32064 [00:00<00:00, 246869.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (32065, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function definition\n",
    "\n",
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int]) -> np.ndarray:\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension))\n",
    "    for word, i in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        # embedding_vector = embedding_model(word)\n",
    "        # if embedding_vector is not None:\n",
    "        #     # words not found in embedding index will be all-zeros.\n",
    "        #     embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, tokenizer.word_index)\n",
    "\n",
    "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length is: 91 words\n",
      "[[ 298. 2296.   76.   26. 1000. 5666.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.]\n",
      " [5280. 4207.  114.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.]\n",
      " [5280. 4207.  860.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "     0.    0.    0.    0.    0.    0.    0.]]\n",
      "X claim train shape:  (121740, 91)\n",
      "X evidence train shape:  (121740, 91)\n",
      "Y train shape:  (121740,)\n",
      "X claim val shape:  (7165, 91)\n",
      "X evidence val shape:  (7165, 91)\n",
      "Y val shape:  (7165,)\n",
      "X claim test shape:  (7189, 91)\n",
      "X evidence test shape:  (7189, 91)\n",
      "Y test shape:  (7189,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def convert_text(texts, tokenizer,max_seq_length):\n",
    "\n",
    "    text_ids = tokenizer.texts_to_sequences(texts)\n",
    "    text_ids = pad_sequences(text_ids, padding='post', truncating='post', maxlen=max_seq_length,dtype='float32')\n",
    "    # text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n",
    "    # text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n",
    "\n",
    "    return text_ids\n",
    "\n",
    "max_seq_length = max(len(x) for x in corpus)\n",
    "print(\"Max sentence length is: {} words\".format(max_seq_length))\n",
    "\n",
    "# Train\n",
    "x_train_Claim = convert_text(x_train_Claim, tokenizer,max_seq_length)\n",
    "x_train_Evidence = convert_text(x_train_Evidence, tokenizer,max_seq_length)\n",
    "\n",
    "print(x_train_Claim[:3])\n",
    "print('X claim train shape: ', x_train_Claim.shape)\n",
    "print('X evidence train shape: ', x_train_Evidence.shape)\n",
    "print('Y train shape: ', y_train.shape)\n",
    "\n",
    "# Val\n",
    "x_val_Claim = convert_text(x_val_Claim, tokenizer,max_seq_length)\n",
    "x_val_Evidence = convert_text(x_val_Evidence, tokenizer,max_seq_length)\n",
    "print('X claim val shape: ', x_val_Claim.shape)\n",
    "print('X evidence val shape: ', x_val_Evidence.shape)\n",
    "print('Y val shape: ', y_val.shape)\n",
    "\n",
    "# Test\n",
    "x_test_Claim = convert_text(x_test_Claim, tokenizer,max_seq_length)\n",
    "x_test_Evidence = convert_text(x_test_Evidence, tokenizer,max_seq_length)\n",
    "print('X claim test shape: ', x_test_Claim.shape)\n",
    "print('X evidence test shape: ', x_test_Evidence.shape)\n",
    "print('Y test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#Cosine similarity\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "\n",
    "def merge_multi_inputs(strategy,input1,input2):\n",
    "\n",
    "  distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([input1, input2])\n",
    "  merge = []\n",
    "  if strategy == 'concatenation':\n",
    "    merge = Concatenate()([input1, input2])\n",
    "  elif strategy == 'sum':\n",
    "    merge = Add()([input1, input2])\n",
    "  elif strategy == 'mean':\n",
    "    merge = Average()([input1, input2])\n",
    "  #concatenate cosine similarity with the claim and evidence\n",
    "  merge = Concatenate()([merge, distance])\n",
    "  return merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lstm: take the last state as the sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model1(merging_stragegy : str,compile_info: Dict) -> keras.Model:\n",
    "\n",
    "    claimInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_claim = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights= [embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_claim\")(claimInput)\n",
    "    eviInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_evi = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_evi\")(eviInput)\n",
    "    Lstm_layer_claim = Bidirectional(LSTM(64,dropout=0.2))(embedding_layer_claim)\n",
    "    Lstm_layer_evi = Bidirectional(LSTM(64,dropout=0.2))(embedding_layer_evi)\n",
    "\n",
    "    merge_data = merge_multi_inputs(merging_stragegy,Lstm_layer_claim,Lstm_layer_evi)\n",
    "   \n",
    "    dense_output1 = Dense(units = 256, activation = \"relu\", name=\"dense_1\")(merge_data)\n",
    "    dense_output2 = Dense(units = 64, activation = \"relu\", name=\"dense_2\")(dense_output1)\n",
    "    dropout_output = Dropout(0.2)(dense_output2)\n",
    "    last_output = Dense(units = 1, activation = \"sigmoid\", name=\"logits\")(dropout_output)\n",
    "\n",
    "    model = Model(inputs=[claimInput,eviInput], outputs=last_output)\n",
    "    \n",
    "    model.summary()\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lstm: average all the output states.?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model2(merging_stragegy : str,compile_info: Dict) -> keras.Model:\n",
    "\n",
    "    claimInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_claim = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights= [embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_claim\")(claimInput)\n",
    "    eviInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_evi = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_evi\")(eviInput)\n",
    "    #return_sequence=True output shape:3d\n",
    "    #return_sequence=flase: 2d\n",
    "    Lstm_layer_claim = Bidirectional(LSTM(64,return_sequences=True,dropout=0.2))(embedding_layer_claim)\n",
    "    Lstm_layer_evi = Bidirectional(LSTM(64,return_sequences=True,dropout=0.2))(embedding_layer_evi)\n",
    "    average_claim = GlobalAveragePooling1D()(Lstm_layer_claim)\n",
    "    average_evi = GlobalAveragePooling1D()(Lstm_layer_evi)\n",
    "    # average_claim = Average()(Lstm_layer_claim)\n",
    "    # average_evi = Average()(Lstm_layer_evi)\n",
    "    # keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1))\n",
    "\n",
    "\n",
    "    merge_data = merge_multi_inputs(merging_stragegy,average_claim,average_evi)\n",
    "   \n",
    "    dense_output1 = Dense(units = 256, activation = \"relu\", name=\"dense_1\")(merge_data)\n",
    "    dense_output2 = Dense(units = 64, activation = \"relu\", name=\"dense_2\")(dense_output1)\n",
    "    dropout_output = Dropout(0.2)(dense_output2)\n",
    "    last_output = Dense(units = 1, activation = \"sigmoid\", name=\"logits\")(dropout_output)\n",
    "\n",
    "    model = Model(inputs=[claimInput,eviInput], outputs=last_output)\n",
    "    \n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP：reshape the 3D input to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(merging_stragegy : str,compile_info: Dict) -> keras.Model:\n",
    "\n",
    "    claimInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_claim = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights= [embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_claim\")(claimInput)\n",
    "    eviInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_evi = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_evi\")(eviInput)\n",
    "    claim_shape = K.shape(claimInput)\n",
    "    evi_shape = K.shape(eviInput)\n",
    "    reshape_claim = Reshape((claim_shape[-2]*claim_shape[-1]))(embedding_layer_claim)\n",
    "    reshape_evi = Reshape((evi_shape[-2]*evi_shape[-1]))(embedding_layer_evi)\n",
    "\n",
    "    merge_data = merge_multi_inputs(merging_stragegy,reshape_claim,reshape_evi)\n",
    "   \n",
    "    dense_output1 = Dense(units = 256, activation = \"relu\", name=\"dense_1\")(merge_data)\n",
    "    dense_output2 = Dense(units = 64, activation = \"relu\", name=\"dense_2\")(dense_output1)\n",
    "    dropout_output = Dropout(0.2)(dense_output2)\n",
    "    last_output = Dense(units = 1, activation = \"sigmoid\", name=\"logits\")(dropout_output)\n",
    "\n",
    "    model = Model(inputs=[claimInput,eviInput], outputs=last_output)\n",
    "    \n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOV:Compute the sentence embedding as the mean of its token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bov_model(merging_stragegy : str,compile_info: Dict) -> keras.Model:\n",
    "\n",
    "    claimInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_claim = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights= [embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_claim\")(claimInput)\n",
    "    eviInput=Input(shape=(max_seq_length,))\n",
    "    embedding_layer_evi = Embedding(input_dim = vocabLength,\n",
    "                                output_dim = 300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_seq_length,\n",
    "                                mask_zero = True,\n",
    "                                name = \"embedding_layer_evi\")(eviInput)\n",
    "    # average_claim = Average()(embedding_layer_claim)\n",
    "    # average_evi = Average()(embedding_layer_evi)\n",
    "    average_claim = GlobalAveragePooling1D()(embedding_layer_claim)\n",
    "    average_evi = GlobalAveragePooling1D()(embedding_layer_evi)\n",
    "\n",
    "    merge_data = merge_multi_inputs(merging_stragegy,average_claim,average_evi)\n",
    "   \n",
    "    dense_output1 = Dense(units = 256, activation = \"relu\", name=\"dense_1\")(merge_data)\n",
    "    dense_output2 = Dense(units = 64, activation = \"relu\", name=\"dense_2\")(dense_output1)\n",
    "    dropout_output = Dropout(0.2)(dense_output2)\n",
    "    last_output = Dense(units = 1, activation = \"sigmoid\", name=\"logits\")(dropout_output)\n",
    "\n",
    "    model = Model(inputs=[claimInput,eviInput], outputs=last_output)\n",
    "    \n",
    "\n",
    "    # Debug\n",
    "    model.summary()\n",
    "    #keras.utils.plot_model(model, \"model.png\", show_shapes=True)\n",
    "    # Compile\n",
    "    model.compile(**compile_info)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 91)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 91)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_claim (Embeddin (None, 91, 300)      9619500     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer_evi (Embedding) (None, 91, 300)      9619500     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 91, 128)      186880      embedding_layer_claim[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 91, 128)      186880      embedding_layer_evi[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 257)          0           concatenate[0][0]                \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          66048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 1)            65          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,695,321\n",
      "Trainable params: 19,695,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "compile_info = {\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'metrics': ['accuracy'],\n",
    "}\n",
    "\n",
    "model = create_lstm_model2('concatenation',compile_info)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training! \n",
      "Parameters: {'verbose': 1, 'epochs': 5, 'batch_size': 256, 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x0000028C0B9DF730>]}\n",
      "Epoch 1/5\n",
      "476/476 [==============================] - 1095s 2s/step - loss: 0.4950 - accuracy: 0.7771 - val_loss: 0.6568 - val_accuracy: 0.6329\n",
      "Epoch 2/5\n",
      "476/476 [==============================] - 1125s 2s/step - loss: 0.4186 - accuracy: 0.8145 - val_loss: 0.6481 - val_accuracy: 0.6751\n",
      "Epoch 3/5\n",
      "476/476 [==============================] - 1111s 2s/step - loss: 0.3707 - accuracy: 0.8355 - val_loss: 0.6467 - val_accuracy: 0.6770\n",
      "Epoch 4/5\n",
      "476/476 [==============================] - 1113s 2s/step - loss: 0.3289 - accuracy: 0.8533 - val_loss: 0.7164 - val_accuracy: 0.6840\n",
      "Epoch 5/5\n",
      "476/476 [==============================] - 1120s 2s/step - loss: 0.2938 - accuracy: 0.8694 - val_loss: 0.7310 - val_accuracy: 0.6863\n",
      "Training completed! Showing history...\n",
      "Displaying the following history keys:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction: \n",
      "{'batch_size': 256, 'verbose': 1}\n",
      "Predicting on 7189 samples\n",
      "29/29 [==============================] - 12s 428ms/step\n"
     ]
    }
   ],
   "source": [
    "def show_history(history: keras.callbacks.History):\n",
    "\n",
    "    history_data = history.history\n",
    "    print(\"Displaying the following history keys: \", history_data.keys())\n",
    "\n",
    "    for key, value in history_data.items():\n",
    "        if not key.startswith('val'):\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.set_title(key)\n",
    "            ax.plot(value)\n",
    "            if 'val_{}'.format(key) in history_data:\n",
    "                ax.plot(history_data['val_{}'.format(key)])\n",
    "            else:\n",
    "                print(\"Couldn't find validation values for metric: \", key)\n",
    "\n",
    "            ax.set_ylabel(key)\n",
    "            ax.set_xlabel('epoch')\n",
    "            ax.legend(['train', 'val'], loc='best')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: keras.Model,\n",
    "                x_train_Claim: np.ndarray,\n",
    "                x_train_Evidence: np.ndarray,\n",
    "                y_train: np.ndarray,\n",
    "                x_val_Claim: np.ndarray,\n",
    "                x_val_Evidence: np.ndarray,\n",
    "                y_val: np.ndarray,\n",
    "                training_info: Dict):\n",
    "\n",
    "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
    "    history = model.fit(x=(x_train_Claim,x_train_Evidence), y=y_train,\n",
    "                        validation_data=([x_val_Claim,x_val_Evidence], y_val),\n",
    "                        **training_info)\n",
    "    print(\"Training completed! Showing history...\")\n",
    "\n",
    "    show_history(history)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_data(model: keras.Model,\n",
    "                 x_test_Claim: np.ndarray,\n",
    "                 x_test_Evidence:np.ndarray,\n",
    "                 prediction_info: Dict) -> np.ndarray:\n",
    "\n",
    "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
    "    print('Predicting on {} samples'.format(x_test_Claim.shape[0]))\n",
    "\n",
    "    predictions = model.predict([x_test_Claim,x_test_Evidence], **prediction_info)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions: np.ndarray,\n",
    "                         y: np.ndarray,\n",
    "                         metrics: List[Callable],\n",
    "                         metric_names: List[str]):\n",
    "\n",
    "    assert len(metrics) == len(metric_names)\n",
    "\n",
    "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
    "\n",
    "    metric_info = {}\n",
    "\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        metric_value = metric(y_pred=predictions, y_true=y)\n",
    "        metric_info[metric_name] = metric_value\n",
    "\n",
    "    return metric_info\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 256,\n",
    "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=10,\n",
    "                                                restore_best_weights=True)]\n",
    "}\n",
    "model = train_model(model=model, x_train_Claim=x_train_Claim, x_train_Evidence=x_train_Evidence,y_train=y_train,\n",
    "                    x_val_Claim=x_val_Claim,x_val_Evidence=x_val_Evidence, y_val=y_val, training_info=training_info)\n",
    "\n",
    "# Inference\n",
    "\n",
    "prediction_info = {\n",
    "    'batch_size': 256,\n",
    "    'verbose': 1\n",
    "}\n",
    "test_predictions = predict_data(model=model, x_test_Claim=x_test_Claim,x_test_Evidence=x_test_Evidence,\n",
    "                                      prediction_info=prediction_info)\n",
    "\n",
    "# Retrieving labels from raw predictions\n",
    "# test_predictions = np.argmax(test_predictions, axis=-1) \n",
    "\n",
    "# Evaluation\n",
    "\n",
    "metrics = [\n",
    "    accuracy_score,\n",
    "    partial(f1_score, pos_label=1, average='binary')\n",
    "]\n",
    "metric_names = [\n",
    "    \"accuracy\",\n",
    "    \"binary_f1\"\n",
    "]\n",
    "metric_info = evaluate_predictions(predictions=test_predictions,\n",
    "                                   y=y_test,\n",
    "                                   metrics=metrics,\n",
    "                                   metric_names=metric_names)\n",
    "\n",
    "print('Metrics info: \\n{}'.format(metric_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# test_predictions = np.argmax(test_predictions, axis=-1) \n",
    "# print(test_predictions[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-input classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     REFUTES       0.50      1.00      0.67      3583\n",
      "    SUPPORTS       1.00      0.00      0.00      3606\n",
      "\n",
      "    accuracy                           0.50      7189\n",
      "   macro avg       0.75      0.50      0.33      7189\n",
      "weighted avg       0.75      0.50      0.33      7189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, test_predictions,zero_division=True,labels=[0,1], target_names=[\"REFUTES\", \"SUPPORTS\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim verification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     REFUTES       0.50      1.00      0.67      3304\n",
      "    SUPPORTS       1.00      0.00      0.00      3309\n",
      "\n",
      "    accuracy                           0.50      6613\n",
      "   macro avg       0.75      0.50      0.33      6613\n",
      "weighted avg       0.75      0.50      0.33      6613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.insert(4,'Prediction',test_predictions)\n",
    "rule = lambda x: x.mode().iat[0]\n",
    "voting_test_label = testData.groupby('ID')['Label'].apply(rule).reset_index(name='Majority_Label')\n",
    "voting_predict_label = testData.groupby('ID')['Prediction'].apply(rule).reset_index(name='Majority_Label')\n",
    "\n",
    "report = classification_report(voting_test_label['Majority_Label'], voting_predict_label['Majority_Label'],zero_division=True,labels=[0,1], target_names=[\"REFUTES\", \"SUPPORTS\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = pd.DataFrame(test_predictions)\n",
    "# prediction.to_csv('E:/ai/2021-2/NATURAL LANGUAGE PROCESSING/assignment/testprediction.csv')   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d6321f44481eb46a73bf81dc16280167257041196987c880b4339fc7b585d76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
